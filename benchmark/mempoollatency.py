import os
import re
from datetime import datetime
from collections import defaultdict

# æ—¥å¿—æ—¶é—´æ ¼å¼
TIME_FORMAT = "%Y/%m/%d %H:%M:%S.%f"

def parse_log_file(log_file):
    """
    è§£æå•ä¸ªæ—¥å¿—æ–‡ä»¶ï¼Œæå– batch åˆ›å»ºæ—¶é—´å’Œæ¥æ”¶æ—¶é—´
    è¿”å›ä¸¤ä¸ª dictï¼š
        - create_times[(creator_node, batch_id)] = datetime
        - receive_times[(creator_node, batch_id)] = [datetime, ...]
    """
    create_pattern = re.compile(r'create Block node (\d+) batch_id (\d+)')
    recv_pattern = re.compile(r'recieve payload from (\d+) and batchid is (\d+)')
    timestamp_pattern = re.compile(r'\[WARN\] ([\d/:.\s]+)')

    create_times = {}
    receive_times = defaultdict(list)

    with open(log_file, 'r', encoding='utf-8') as f:
        for line in f:
            # æå–æ—¶é—´æˆ³
            time_match = timestamp_pattern.match(line)
            if not time_match:
                continue
            timestamp = datetime.strptime(time_match.group(1).strip(), TIME_FORMAT)

            # åŒ¹é… create Block
            create_match = create_pattern.search(line)
            if create_match:
                node_id = int(create_match.group(1))
                batch_id = int(create_match.group(2))
                create_times[(node_id, batch_id)] = timestamp

            # åŒ¹é… recieve payload
            recv_match = recv_pattern.search(line)
            if recv_match:
                node_id = int(recv_match.group(1))
                batch_id = int(recv_match.group(2))
                receive_times[(node_id, batch_id)].append(timestamp)

    return create_times, receive_times

def merge_batch_data(log_files):
    """
    ä»å¤šä¸ªæ—¥å¿—æ–‡ä»¶ä¸­åˆå¹¶ batch åˆ›å»ºå’Œæ¥æ”¶æ—¶é—´
    """
    all_create_times = {}
    all_receive_times = defaultdict(list)

    for log_file in log_files:
        create_times, receive_times = parse_log_file(log_file)
        all_create_times.update(create_times)
        for key, times in receive_times.items():
            all_receive_times[key].extend(times)

    return all_create_times, all_receive_times

def compute_delays(create_times, receive_times):
    """
    è®¡ç®—æ¯ä¸ª batch çš„å¹³å‡æ¥æ”¶å»¶è¿Ÿä»¥åŠæ•´ä½“å¹³å‡å»¶è¿Ÿ
    """
    total_delay = 0.0
    total_count = 0

    print("ğŸ“Š æ¯ä¸ª Batch çš„å¹³å‡æ¥æ”¶å»¶è¿Ÿï¼š")
    print("Batch (creator, id) | Avg Delay (s) | Num Received")
    print("--------------------|----------------|---------------")

    for key in sorted(create_times.keys()):
        create_time = create_times[key]
        recv_list = receive_times.get(key, [])
        batch_str = f"({key[0]}, {key[1]})"
        if recv_list:
            delays = [(recv_time - create_time).total_seconds() for recv_time in recv_list]
            avg_delay = sum(delays) / len(delays)
            total_delay += sum(delays)
            total_count += len(delays)
           # print(f"{batch_str:<20} | {avg_delay:<14.6f} | {len(recv_list)}")
       # else:
         #   print(f"{batch_str:<20} | No receive data  | 0")

    if total_count > 0:
        overall_avg = total_delay / total_count
        print("\nâœ… å…¨éƒ¨æ¥æ”¶äº‹ä»¶çš„å¹³å‡å»¶è¿Ÿï¼š")
        print(f"æ€»æ¥æ”¶æ¬¡æ•°: {total_count}")
        print(f"å¹³å‡å»¶è¿Ÿ: {overall_avg:.6f} ç§’")
    else:
        print("\nâš ï¸ æ— ä»»ä½•æ¥æ”¶äº‹ä»¶ï¼Œæ— æ³•è®¡ç®—å¹³å‡å»¶è¿Ÿã€‚")

if __name__ == '__main__':
    log_dir = 'logs/2025-05-29v15-00-55'
    log_files = [
        os.path.join(log_dir, f'node-warn-{i}.log') for i in range(4)
    ]

    print("ğŸ” æ­£åœ¨è§£ææ‰€æœ‰æ—¥å¿—æ–‡ä»¶...\n")
    create_times, receive_times = merge_batch_data(log_files)
    compute_delays(create_times, receive_times)
